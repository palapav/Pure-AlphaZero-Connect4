playing marks in one column -> we still have moves geared towards that column
once filled -> need to check if this resolved (test)

unit testing -> statement, boundary, path, stress

Jun 15th update:
Read AlphaZero paper -> all detailed

* All changes going forward needs to be done in a branch
and merged into main (currently doing this)

* Changes to be made

* Refactor NN so that training data and MCTS only take 2D tensors (done)

* refactor root node -> to be owned by player one
    * wins are backpropagated to root node -> check with pure MCTS

* refactor optimal z score in backpropagation and training data -> total z score / total visits (done)

# np.random.choice for root policy vector

* stop illegal moves in eval mode

* to test mcts using untrained neural network -> 3 in a row -> can we get to 4 (done)

# cross entropy -> zero sum property

# check whether policy and chosen actions are lining up index wise

* total z value / total visits (done)

* temperature parameter -> take all probabiltiies and raise to one over temperature power
if temperature is very high -> policy very explorative

* refactor out for loops

* print board states, policy states, and values

* add regularization to loss function

* casting to double in loss function -> see if there is a better way

* need to remove reward/terminal score attribute (done -> using is_finished)

TESTING PARADIGM
- need to test at the functional level
- then piecing all the functions together to form a working program (mcts)
- then need to test single game
- then test entire train pipeline
- as abstraction gets larger -> testing needs to scale at the same time
- so we can better isolate problems and have better defined inputs and outputs

* check views versus copies

* does convert_arr need to be a static method?

# refactor Node class -> create smaller objects -> too many parameters

* when refactoring root node to be owned by player who is about to make a move on the board 
-> current problem -> need to change expansion still in old way (done)

* need to do a deep analysis of why backpropagation is working in the opposite direction than expected
  - is board ownership not valued as much as current representation of state (with played move)
  - did fixing z score at root node help?